{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9928b635",
   "metadata": {},
   "source": [
    "# Foundation of Deep Learning programming exercise 1\n",
    "\n",
    "In this exercise, we will run an SGD code on a simple neural work and examine the convergence of the weight parameters. \n",
    "Part of the codes in this notebook has been written for you; and part of them will be filled up by you. After each question in the markdown cell, replace the blank in the code cell by a correct code and delete the first # sign in the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcd904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from sklearn.datasets import load_diabetes\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid #sigmoid function as activation function\n",
    "import IPython\n",
    "print(\"packages imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99156397",
   "metadata": {},
   "source": [
    "## I. Load Dataset \n",
    "The following dataset includes 10 features like age, sex, body mass index,... obtained for diabetes patients in baseline, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "Note that each of these feature variables have been mean centered and rescaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96426ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes=load_diabetes() # load dataset\n",
    "X=diabetes['data'].astype(np.float32) #inpout\n",
    "Y=diabetes['target'].astype(np.float32) #output\n",
    "X_th=th.from_numpy(X)\n",
    "Y_th=th.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931bac3",
   "metadata": {},
   "source": [
    "1. How many features and samples are there?  Hint: Use the .shape function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d258099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "#################################\n",
    "\n",
    "#d_in = ___ #number of features\n",
    "#n = ___ #number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ae4ce",
   "metadata": {},
   "source": [
    "## II. Define Loss Function\n",
    "We define the loss function to be the mean square error (MSE):\n",
    "$$ \\ell(\\mathbf{y}_\\text{pred},\\mathbf{y}) := \\frac1N \\|\\mathbf{y}_\\text{pred}-\\mathbf{y}\\|_2^2 $$\n",
    "where $\\mathbf{y} \\in \\mathbb{R}^n$ is the output vector and $\\mathbf{y}_\\text{pred}$ its prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ee25f",
   "metadata": {},
   "source": [
    "2. Define the loss function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3270e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "#################################\n",
    "\n",
    "#def np_mse_loss(y_pred,y,n):\n",
    "#  return ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994075a",
   "metadata": {},
   "source": [
    "There is a built-in function for MSE in Pytorch. Let's check if the answer in Question 2 is correct by running the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e00735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 verification\n",
    "#################################\n",
    "\n",
    "th_mse_loss=th.nn.MSELoss()\n",
    "test_vector_1 = np.random.random(-1,1,n)\n",
    "test_vector_2 = np.random.random(-1,1,n)\n",
    "check = (np_mse_loss(test_vector_1,test_vector_2,n) == th_mse_loss(test_vector_1,test_vector_2,n))\n",
    "if check == 1:\n",
    "    print(\"Definition of MSE is correct.\")\n",
    "else:\n",
    "    print(\"Definition of MSE is incorrect. Please redo Question 2 again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24c867",
   "metadata": {},
   "source": [
    "## III. Define MLP\n",
    "Here we define the same shallow neural network (with initialization) in PyTorch and Numpy respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "class th_MLP(th.nn.Module):\n",
    "    def __init__(self,h,d):\n",
    "        super(th_MLP, self).__init__()\n",
    "        self.theta = th.nn.Linear(d, h,bias=False) #h = width of layer\n",
    "        self.beta = th.nn.Linear(h, 1,bias=False)\n",
    "        self.act=th.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.beta(self.act(self.theta(x))).squeeze(-1)\n",
    "\n",
    "# Numpy\n",
    "class np_MLP():\n",
    "    def __init__(self,h,d,th_model):\n",
    "        super(np_MLP, self).__init__()\n",
    "        self.theta = th_model.theta.weight.detach().numpy() # we copy the weights to have the exact same model!\n",
    "        self.beta= th_model.beta.weight.detach().numpy() # \n",
    "    def forward(self, x):\n",
    "        return np.dot(self.beta,sigmoid(np.dot(self.theta,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2869365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP verification\n",
    "#################################\n",
    "\n",
    "h,d=25,d_in\n",
    "th_model=th_MLP(h,d)\n",
    "y_pred_th=th_model.forward(X_th)\n",
    "\n",
    "np_model=np_MLP(h,d,th_model)\n",
    "y_pred_np=np_model.forward(X.T)\n",
    "\n",
    "print(f\"Prediction shape {y_pred_np.shape}\")\n",
    "print(\" \")\n",
    "print(f\"Numpy loss: {np_mse_loss(y_pred_np,Y,n)}\")\n",
    "print(f\"Torch loss: {th_mse_loss(y_pred_th,Y_th).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac939592",
   "metadata": {},
   "source": [
    "# IV. Define Gradient\n",
    "In order to do SGD, first we need to compute the derivative of the Sigmoid function: $\\sigma'(x)$ where $\\sigma(x):= \\frac{1}{1+e^{-x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1de3a",
   "metadata": {},
   "source": [
    "3. Define the derivative of the Sigmoid function. Hint: You can use the pre-defined Sigmoid function $\\backslash\\texttt{sigmoid(x)}$ in the expression of $\\sigma'(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57affc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "#################################\n",
    "\n",
    "#def sigmoid_prime(x): #partial derivative of Sigmoid\n",
    "#  return ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80e7e9",
   "metadata": {},
   "source": [
    "The gradient of the weights has been defined for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe62d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_prime(y_pred,y): #partial derivative of MSE\n",
    "    return -2*(y-y_pred)  # [1 x n] \n",
    "\n",
    "def np_mse_grad_beta(x,y_pred,y,n): # total derivative output weights\n",
    "    return 1/n*np.dot(sigmoid(np.dot(np_model.theta,x.T)),l_prime(y_pred,y).T) # [1 x h]\n",
    "\n",
    "def np_mse_grad_theta(x,y_pred,y,n): # total derivative input weights\n",
    "    theta_x=np.dot(np_model.theta,x.T) # [h x n]\n",
    "    phi_prime=sigmoid_prime(theta_x) # [h x n]\n",
    "    return 1/n*np.dot(np.outer(np_model.beta,l_prime(y_pred,y))*phi_prime,x) # [d x h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfb848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch: Autodifferentiation\n",
    "loss=th_mse_loss(y_pred_th,Y_th)\n",
    "grad_beta=th.autograd.grad(loss,th_model.beta.weight,retain_graph=True)\n",
    "grad_theta=th.autograd.grad(loss,th_model.theta.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29719c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 verification\n",
    "#################################\n",
    "\n",
    "print(np.allclose(np_mse_grad_beta(X,y_pred_np,Y,n).T,grad_beta[0].numpy()))\n",
    "print(np.allclose(np_mse_grad_theta(X,y_pred_np,Y,n),grad_theta[0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c30022",
   "metadata": {},
   "source": [
    "# V. SGD\n",
    "\n",
    "Now we run SGD on the loss function and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of steps and learning rate\n",
    "#################################\n",
    "\n",
    "steps,lr=200,0.0005 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_np,losses_th=[],[]\n",
    "optimizer = th.optim.SGD(th_model.parameters(), lr=lr) \n",
    "\n",
    "for k in range(steps):\n",
    "    # numpy\n",
    "    y_pred_np=np_model.forward(X.T)\n",
    "    losses_np.append(np_mse_loss(y_pred_np,Y,n))\n",
    "    np_model.beta=np_model.beta-lr*np_mse_grad_beta(X,y_pred_np,Y,n).T\n",
    "    np_model.theta=np_model.theta-lr*np_mse_grad_theta(X,y_pred_np,Y,n)\n",
    "\n",
    "    # torch\n",
    "    y_pred_th=th_model.forward(X_th)\n",
    "    loss = th_mse_loss(y_pred_th, Y_th)\n",
    "    losses_th.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "plt.plot(losses_np,label=\"Numpy\")\n",
    "plt.plot(losses_th,label=\"PyTorch\",linestyle='dashed')\n",
    "plt.title(\"Loss over steps (epochs)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62c8d4",
   "metadata": {},
   "source": [
    "Do you see a convergence curve in the plot? What happens if we change the number of steps and learning rate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d49c40ad3b04ef902d02372ed435b112d884ca92abec04ab503cc2d9161c5693"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
